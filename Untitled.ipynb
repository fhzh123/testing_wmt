{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b2583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"  # Set the GPU 2 to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a973af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "# Import custom modules\n",
    "from training import training\n",
    "# Utils\n",
    "from utils import str2bool, path_check, set_random_seed\n",
    "\n",
    "user_name = os.getlogin()\n",
    "parser = argparse.ArgumentParser(description='Parsing Method')\n",
    "# Task setting\n",
    "parser.add_argument('--training', action='store_true')\n",
    "parser.add_argument('--resume', action='store_true')\n",
    "parser.add_argument('--debuging_mode', action='store_true')\n",
    "# Path setting\n",
    "parser.add_argument('--preprocess_path', default=f'/HDD/{user_name}/preprocessed', type=str,\n",
    "                    help='Pre-processed data save path')\n",
    "parser.add_argument('--data_path', default='/HDD/dataset/WMT/2016/multi_modal', type=str,\n",
    "                    help='Original data path')\n",
    "parser.add_argument('--model_save_path', default=f'/HDD/{user_name}/model_checkpoint/acl_text_aug', type=str,\n",
    "                    help='Model checkpoint file path')\n",
    "parser.add_argument('--result_path', default=f'/HDD/{user_name}/results/acl_text_aug', type=str,\n",
    "                    help='Results file path')\n",
    "# Training setting\n",
    "parser.add_argument('--min_len', default=4, type=int, \n",
    "                    help=\"Sentences's minimum length; Default is 4\")\n",
    "parser.add_argument('--src_max_len', default=150, type=int, \n",
    "                    help=\"Source sentences's maximum length; Default is 150\")\n",
    "parser.add_argument('--trg_max_len', default=150, type=int, \n",
    "                    help=\"Target sentences's maximum length; Default is 150\")\n",
    "parser.add_argument('--num_epochs', default=100, type=int, \n",
    "                    help='Training epochs; Default is 100')\n",
    "parser.add_argument('--num_workers', default=8, type=int, \n",
    "                    help='Num CPU Workers; Default is 8')\n",
    "parser.add_argument('--batch_size', default=16, type=int,    \n",
    "                    help='Batch size; Default is 16')\n",
    "parser.add_argument('--lr', default=5e-5, type=float,\n",
    "                    help='Maximum learning rate of warmup scheduler; Default is 5e-5')\n",
    "parser.add_argument('--w_decay', default=1e-5, type=float,\n",
    "                    help=\"Ralamb's weight decay; Default is 1e-5\")\n",
    "parser.add_argument('--clip_grad_norm', default=5, type=int, \n",
    "                    help='Graddient clipping norm; Default is 5')\n",
    "parser.add_argument('--label_smoothing_eps', default=0.05, type=float,\n",
    "                    help='')\n",
    "# Testing setting\n",
    "parser.add_argument('--test_batch_size', default=32, type=int, \n",
    "                    help='Test batch size; Default is 32')\n",
    "parser.add_argument('--beam_size', default=5, type=int, \n",
    "                    help='Beam search size; Default is 5')\n",
    "parser.add_argument('--beam_alpha', default=0.7, type=float, \n",
    "                    help='Beam search length normalization; Default is 0.7')\n",
    "parser.add_argument('--repetition_penalty', default=1.3, type=float, \n",
    "                    help='Beam search repetition penalty term; Default is 1.3')\n",
    "# Seed & Logging setting\n",
    "parser.add_argument('--seed', default=42, type=int,\n",
    "                    help='Random seed; Default is 42')\n",
    "parser.add_argument('--use_tensorboard', default=True, type=str2bool,\n",
    "                    help='Using tensorboard; Default is True')\n",
    "parser.add_argument('--tensorboard_path', default='./tensorboard_runs', type=str,\n",
    "                    help='Tensorboard log path; Default is ./tensorboard_runs')\n",
    "parser.add_argument('--print_freq', default=100, type=int, \n",
    "                    help='Print training process frequency; Default is 100')\n",
    "args = parser.parse_args(list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6762349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from dataset import CustomDataset\n",
    "from model import TransformerModel\n",
    "from utils import TqdmLoggingHandler, write_log, get_tb_exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17a345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2023-05-02 20:33:33 - Start training!\n",
      " 2023-05-02 20:33:33 - Load data...\n",
      " 2023-05-02 20:33:33 - Data loading done!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#===================================#\n",
    "#==============Logging==============#\n",
    "#===================================#\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler = TqdmLoggingHandler()\n",
    "handler.setFormatter(logging.Formatter(\" %(asctime)s - %(message)s\", \"%Y-%m-%d %H:%M:%S\"))\n",
    "logger.addHandler(handler)\n",
    "logger.propagate = False\n",
    "\n",
    "write_log(logger, 'Start training!')\n",
    "\n",
    "#===================================#\n",
    "#=============Data Load=============#\n",
    "#===================================#\n",
    "\n",
    "write_log(logger, \"Load data...\")\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "src_list = dict()\n",
    "trg_list = dict()\n",
    "\n",
    "# 3) Test data load\n",
    "with open(os.path.join(args.data_path, 'test.de'), 'r') as f:\n",
    "    src_list['test'] = [x.replace('\\n', '') for x in f.readlines()]\n",
    "with open(os.path.join(args.data_path, 'test.en'), 'r') as f:\n",
    "    trg_list['test'] = [x.replace('\\n', '') for x in f.readlines()]\n",
    "\n",
    "write_log(logger, 'Data loading done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b164ceb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2023-05-02 20:33:33 - Instantiating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bart to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (encoder_model): BartEncoder(\n",
       "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (latent_encoder): Linear(in_features=768, out_features=384, bias=True)\n",
       "  (latent_decoder): Linear(in_features=384, out_features=768, bias=True)\n",
       "  (decoder_model): BartDecoder(\n",
       "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder_linear): Linear(in_features=768, out_features=384, bias=True)\n",
       "  (decoder_norm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "  (decoder_linear2): Linear(in_features=384, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#===================================#\n",
    "#===========Train setting===========#\n",
    "#===================================#\n",
    "\n",
    "# 1) Model initiating\n",
    "write_log(logger, 'Instantiating model...')\n",
    "model = TransformerModel()\n",
    "save_file_name = os.path.join(args.model_save_path, f'checkpoint.pth.tar')\n",
    "checkpoint = torch.load(save_file_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ec494f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.12.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe4f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')\n",
    "test_dataset = CustomDataset(tokenizer=tokenizer,\n",
    "                           src_list=src_list['test'], trg_list=trg_list['test'],\n",
    "                           src_max_len=args.src_max_len, trg_max_len=args.trg_max_len)\n",
    "test_loader = DataLoader(test_dataset, drop_last=False,\n",
    "                        batch_size=4, shuffle=True,\n",
    "                        pin_memory=True, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7de417ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch_iter in enumerate(test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Input, output setting\n",
    "    src_sequence = batch_iter[0]\n",
    "    src_att = batch_iter[1]\n",
    "    src_sequence = src_sequence.to(device, non_blocking=True)\n",
    "    src_att = src_att.to(device, non_blocking=True)\n",
    "\n",
    "    trg_sequence = batch_iter[2]\n",
    "    trg_att = batch_iter[3]\n",
    "    trg_sequence = trg_sequence.to(device, non_blocking=True)\n",
    "    trg_att = trg_att.to(device, non_blocking=True)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d89909d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Encoding\n",
    "    encoder_out = model.encoder_model(input_ids=src_sequence,\n",
    "                                     attention_mask=src_att)\n",
    "    encoder_out = encoder_out['last_hidden_state'] # (batch_size, seq_len, d_hidden)\n",
    "\n",
    "    # Latent setting\n",
    "    encoder_out = model.latent_encoder(encoder_out)\n",
    "    encoder_out = model.latent_decoder(encoder_out)\n",
    "\n",
    "    # Input, output setting\n",
    "    batch_size = encoder_out.size(0)\n",
    "    src_seq_size = encoder_out.size(1)\n",
    "\n",
    "    # Decoding start token setting\n",
    "    seqs = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "    seqs = seqs.repeat(batch_size, 1).contiguous() # (batch_size, 1)\n",
    "\n",
    "    for step in range(model.src_max_len):\n",
    "        # Decoding sentence\n",
    "        decoder_outputs = model.decoder_model(\n",
    "            input_ids=seqs,\n",
    "            encoder_hidden_states=encoder_out,\n",
    "            encoder_attention_mask=src_att\n",
    "        )\n",
    "        decoder_outputs = decoder_outputs['last_hidden_state']\n",
    "\n",
    "        # Next word probability\n",
    "        scores = F.gelu(model.decoder_linear(decoder_outputs[:,-1])) # (batch_size, d_embedding)\n",
    "        scores = model.decoder_linear2(model.decoder_norm(scores)) # (batch_size, vocab_num)\n",
    "\n",
    "        next_word_prob = F.softmax(scores, dim=1) # (batch_size, vocab_num)\n",
    "        next_word = torch.argmax(next_word_prob, dim=-1)\n",
    "\n",
    "        # Concatenate generated token to sequence\n",
    "        next_word = next_word.unsqueeze(1) # (batch_size, 1)\n",
    "        seqs = torch.cat([seqs, next_word], dim=1) # (batch_size, seq_len + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017fc811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[35110, 40720, 28937, 40103, 23318,  8988, 14785, 19144, 17854, 37545,\n",
       "         18714, 35765, 50054,  5649,  9220,  9220, 12130,  8266, 10888, 18989,\n",
       "         44837,  3638, 36088,  4028, 45267, 11811, 22293,  9374,  9220, 28937,\n",
       "         36088,  8987, 12130, 36933, 10888, 23959,  9902,  5027, 13516, 20349,\n",
       "          9374, 13516, 44837,  6018, 44070, 50208, 36858, 16300, 20642, 18714,\n",
       "         19112, 12130,  4106, 42260, 10888, 24998, 45145,  2502, 35355,  7038,\n",
       "         10852,  7038, 29746, 26974, 21657, 42253, 35355, 31918, 42199,  3507,\n",
       "         23409,  7133, 25389, 43064, 49779, 12130, 29256, 12130, 29256, 12130,\n",
       "         29256, 12130, 29256, 12130, 29256, 12130, 29256, 12130, 29256, 12130,\n",
       "         29256, 12130, 29256, 12130, 29256, 12130, 29256, 12130, 29256, 12130,\n",
       "         29256, 12130, 29256, 12130, 29256, 12130, 29256, 12130, 29256, 12130,\n",
       "         29256, 12130, 29256, 12130, 29256, 12130, 29256, 12130, 29256, 12130,\n",
       "         29256, 17815, 17815, 26946, 24753,  8103, 45267, 39173, 14092,  3823,\n",
       "          8088,  7133, 17085, 40227, 21756,  7052, 10757, 37097, 46084,  7133,\n",
       "         32915, 46084,  7133,  6565, 35264, 31846, 21613, 18196,  9902,  6109],\n",
       "        [35110, 40720,  4366, 25769, 29746,  9900, 12905, 32703, 49352,   150,\n",
       "         30186, 15438, 32598,  7133,  7133,  7133, 24409, 17815,  8206, 28866,\n",
       "         49613, 22205,  7133, 47067,  1107, 28222,  9535, 13516, 43883, 36804,\n",
       "         49358, 25891, 35676, 50020,  9590, 10757, 13198, 46084,  7133, 36804,\n",
       "          7133, 36804, 49358, 25891,  7251,  1107,  1107,  1107,  1107, 48642,\n",
       "         32992, 33436,  4913, 44802, 41953, 39981, 48133, 31764,  7133, 36804,\n",
       "         49358, 47914,  3352,  7133, 42260, 24091,  1107,  1107, 48642, 44837,\n",
       "          5564,  4442, 38183, 27579,  7133, 42260, 19037, 17815,  1107,  1107,\n",
       "          1107, 17815,  1107,  1107, 17815,  1107,  1107, 17815,  1107, 10908,\n",
       "         25891, 14785, 38431, 11561, 17815,  4442, 23773, 35045, 40599, 11561,\n",
       "         17815,  4442, 23773, 10757,  4442, 23773, 17815,  4442, 23773, 17815,\n",
       "          4442, 23773, 17815,  4442, 23773, 17815,  4442, 23773, 17815,  4442,\n",
       "         23773, 17815,  4442, 23773, 17815,  4442, 23773, 17815,  4442, 11761,\n",
       "         10908,  9902, 22205,  4442,  7133, 14760, 15764,  1107,  1107, 17815,\n",
       "          1107,  1107, 17815,  1107,  9902,  9902, 20328,   997, 24590, 44287],\n",
       "        [35110, 40720, 24409,  8988, 14785, 32784, 45884, 17596, 27167, 41429,\n",
       "         17735,  5564, 17171, 14785, 35059,  8788, 13410, 48642, 16657, 14785,\n",
       "         32598, 35676, 45424, 46426, 14785, 35059, 18865, 18865,  8358, 44802,\n",
       "          6109, 27167, 30784, 48467,  7133,  6847, 42524, 13293, 10344, 10888,\n",
       "         18989, 44837,  3638, 44802, 48553, 17815, 42623, 23890, 47968, 49813,\n",
       "         37113,  6847, 27800, 26000, 30186, 11811, 25818, 44802, 41953, 23258,\n",
       "          7077, 27167, 29256, 10888, 16850, 19112, 12130,  3443, 26946, 25831,\n",
       "         16850, 24977,  4992, 23890, 16850, 24977, 44802, 13020, 26946, 43872,\n",
       "         38679, 43864,  6847, 26946, 43872, 38679,  7640, 39546, 42473,  4913,\n",
       "         38170, 41085, 39309, 43064, 49813, 23258,  7077, 24015, 17815, 26946,\n",
       "         29671,  6847, 27800, 29256,  4913, 26946,  2129, 42676, 26946,  2129,\n",
       "          2129,  2129,  2129, 25626, 42676, 17815, 26946, 42623, 26946, 49597,\n",
       "         17815, 26946, 42623, 34179, 33111, 29141, 49613, 40991, 32430, 19112,\n",
       "         11811, 11561, 48467, 29671,  2129,  2129,  2129,  2129,  2129,  2129,\n",
       "          2129,  2129,  2129,  2129,  2129, 26063,  9902, 11409, 19112, 40688],\n",
       "        [35110, 40281, 29905, 13305,  2795, 40814,  4366, 10888, 49248, 24891,\n",
       "         42866, 24891, 42866, 24891,  3133, 24153, 45259, 49976, 17085, 16033,\n",
       "         32598, 33436, 24393, 40814,  4366, 43321, 22467, 17815,   504, 19906,\n",
       "         29671, 11360,  9902, 18865, 18865, 42623, 23890, 24393, 18865, 42623,\n",
       "         49984,  8002, 49224, 19112,  9220, 31569, 42623, 44943, 14469,  7133,\n",
       "         44837, 44837,  3638,  5039, 40359, 35179, 10852, 44802,  6109, 13516,\n",
       "         44837, 44837,  3638,  5373, 33327, 13949, 46742,   161,  7133, 16850,\n",
       "         39013, 19112, 29746, 26974, 25750, 32611,  7133, 16850, 16850, 16850,\n",
       "         18548, 36698, 39131, 22065, 41626, 30477, 20409, 19112,  4445, 45424,\n",
       "         41085, 45840, 20393, 18865, 18865, 18865, 18865,  8358, 20393, 13001,\n",
       "         18865, 18865, 18865, 18865,  8358, 20393,  2569,  6847, 42623,  5773,\n",
       "         36165, 24393,  5649,  4235, 13001,  2616, 18865, 31569, 20393, 31569,\n",
       "         44837, 44837, 44837, 12697, 11811, 18865, 18865, 31569, 20393, 31569,\n",
       "          1674, 17815, 42623, 25813,  8103, 45328, 32992, 31569, 29671, 25501,\n",
       "         25389, 13517, 27907, 40142, 30356,  9902, 41307, 42125, 31569, 44837]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6907a6ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> VersionendmentooksLearn ripeacedmandOct reversalTam HooverPACSTDOUTeman Logan Logan Wor ignored Ly Watt Tyrann benchScore Eachmop quarterbacks pulse mer LoganooksScorejust WorOVER Lymarksboy Navy dat Cath mer dat TyrannMC spont\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Hamb proactive orchestra Hooverarant Wor culturalRECT Ly IBvag applicationFal CSearing CS Increased infuri ConcmodeFal Emperor________________ Yet genres visual indust truncRuntime Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Wor directives Laboratory Laboratory unmanned Scan �mopGiving composed Allen intersection visualNR piercedtechn LynchgmentjoinedEmail visualothesEmail visual deserve stool Worst Rouse cuisineboy NASA',\n",
       " '<s> Versionendment gatheredclad Increased´`Second trave while persever Alvarezcredit visual visual visual obese Laboratory lowered ATTール Language visualalion earnings 169 radar dat Symbol shockingly ]; rehabilit Maul\":\"\"},{\" bassgment),\"Email visual shockingly visual shockingly ]; rehabilit fighter earnings earnings earnings earnings surv synergy customization ReadingJenn buffsitimate docker demeanor visual shockingly ];oğ expects visualRECTducers earnings earnings surv Tyrannual extendō detonated visualRECT township Laboratory earnings earnings earnings Laboratory earnings earnings Laboratory earnings earnings Laboratory earnings Select rehabilitmandulty Restaurant Laboratory extend resideigm legions Restaurant Laboratory extend residegment extend reside Laboratory extend reside Laboratory extend reside Laboratory extend reside Laboratory extend reside Laboratory extend reside Laboratory extend reside Laboratory extend reside Laboratory extend Prix Selectboy Language extend visualrecord attitudes earnings earnings Laboratory earnings earnings Laboratory earningsboyboyDid warawareColor',\n",
       " '<s> Versionendment obeseacedmand Gon Sparkle spectators Yar Iro colonual mandatedmand perjury safer beats survussiemandcredit MaulCalling ›mand perjuryosoosoratedJenn NASA YarographersConnection visuallaninitewritingrough Ly Watt Tyrann benchJennutonium Laboratoryatioreditation Zup undermin atomlan Quite cameo persever quarterbacks medievalJenn buffs Peyton Bangladesh Yar directives Ly Mesaarant Worages unmannedupe Mesa stickersmoreditation Mesa stickersJenn Bul unmannedNevertheless 670 ebooklan unmannedNevertheless 670 advisory broomescent ReadingessmentAff depletion trunc undermin Peyton Bangladeshographies Laboratory unmanned nascentlan Quite directives Reading unmanned poorkeley unmanned poor poor poor poorabiliakeley Laboratory unmannedatio unmannedumbledore Laboratory unmannedatio Lepreviewed CAMールvimirtyarant quarterbacks RestaurantConnection nascent poor poor poor poor poor poor poor poor poor poor poorATSboyviaarantadult',\n",
       " '<s> Version Psychologicalessing piano Sheriff errone gathered Ly={ languimmune languimmune langu passing thwartindalヴNRSancredit customization hierarchy errone gatheredال rulings Laboratory 18aught nascent abilitiesboyosoosoatioreditation hierarchyosoatio魔 esteigguratarant Logan inventoratio popupHT visual Tyrann Tyrann benchhr Urug MODearingJenn NASA dat Tyrann Tyrann bench crazy sophisticationApril Unle says visual Mesa Angerarant Increased infuriangan Compliance visual Mesa Mesa Mesa Cream THEYJOCLUSRG Quantumowanarant hedgeCallingAff destruct configurationosoosoosoosorated configurationraftosoosoosoosorated configuration coldlanatio 250 magnesium hierarchyemanimeraft Bowloso inventor configuration inventor Tyrann Tyrann Tyranngoal quarterbacksosooso inventor configuration inventor Committee Laboratoryatioffe �gz synergy inventor nascent Devil indust nail deem Goodbye defencesboyierre mono inventor Tyrann']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.batch_decode(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a0a890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "model2 = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d49c5c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartModel(\n",
       "  (shared): Embedding(50265, 768, padding_idx=1)\n",
       "  (encoder): BartEncoder(\n",
       "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77129198",
   "metadata": {},
   "source": [
    "from transformers import PretrainedConfig, AutoModel, AutoTokenizer\n",
    "model = AutoModel.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e809e7",
   "metadata": {},
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1fb2b",
   "metadata": {},
   "source": [
    "model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728e839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
